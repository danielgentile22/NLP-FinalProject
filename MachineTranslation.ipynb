{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Italian sentences: 1909116\n",
      "Total English sentences: 1909116\n",
      "Italian sentence 1: Ripresa della sessione\n",
      "English sentence 1: Resumption of the session\n",
      "\n",
      "Italian sentence 2: Dichiaro ripresa la sessione del Parlamento europeo, interrotta venerdì 17 dicembre e rinnovo a tutti i miei migliori auguri nella speranza che abbiate trascorso delle buone vacanze.\n",
      "English sentence 2: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "\n",
      "Italian sentence 3: Come avrete avuto modo di constatare il grande \"baco del millennio\" non si è materializzato. Invece, i cittadini di alcuni nostri paesi sono stati colpiti da catastrofi naturali di proporzioni davvero terribili.\n",
      "English sentence 3: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "\n",
      "Italian sentence 4: Avete chiesto che si tenesse una discussione su tale tema nei prossimi giorni, nel corso della presente tornata.\n",
      "English sentence 4: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "\n",
      "Italian sentence 5: Nel frattempo è mio desiderio, come del resto mi è stato chiesto da alcuni colleghi, osservare un minuto di silenzio in memoria di tutte le vittime delle tempeste che si sono abbattute sui diversi paesi dell' Unione europea.\n",
      "English sentence 5: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "italian_file_path = 'europarl-v7.it-en.it'  # Adjust the path if your file is in a different directory\n",
    "english_file_path = 'europarl-v7.it-en.en'\n",
    "\n",
    "def load_sentences(file_path):\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        sentences = file.read().split('\\n')\n",
    "    return sentences\n",
    "\n",
    "italian_sentences = load_sentences(italian_file_path)\n",
    "english_sentences = load_sentences(english_file_path)\n",
    "\n",
    "# Test data loading\n",
    "print(f\"Total Italian sentences: {len(italian_sentences)}\")\n",
    "print(f\"Total English sentences: {len(english_sentences)}\")\n",
    "\n",
    "# Print the first 5 sentences in both languages to check\n",
    "for i in range(5):\n",
    "    print(f\"Italian sentence {i+1}: {italian_sentences[i]}\")\n",
    "    print(f\"English sentence {i+1}: {english_sentences[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, trg_file, src_tokenizer, trg_tokenizer):\n",
    "        self.src_sentences = open(src_file, encoding='utf-8').read().split('\\n')\n",
    "        self.trg_sentences = open(trg_file, encoding='utf-8').read().split('\\n')\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sample = self.src_tokenizer(self.src_sentences[idx])\n",
    "        trg_sample = self.trg_tokenizer(self.trg_sentences[idx])\n",
    "        return torch.tensor(src_sample), torch.tensor(trg_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and vocabulary building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "src_tokenizer = get_tokenizer('spacy', language='it_core_news_sm')\n",
    "trg_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Load sentences (for building vocab)\n",
    "src_sentences = open('europarl-v7.it-en.it', encoding='utf-8').read().split('\\n')\n",
    "trg_sentences = open('europarl-v7.it-en.en', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "# Build vocabularies\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(src_sentences, src_tokenizer), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "trg_vocab = build_vocab_from_iterator(yield_tokens(trg_sentences, trg_tokenizer), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "src_vocab.set_default_index(src_vocab[\"<unk>\"])\n",
    "trg_vocab.set_default_index(trg_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_item, trg_item in batch:\n",
    "        src_batch.append(torch.cat([torch.tensor([src_vocab[\"<bos>\"]]), src_item, torch.tensor([src_vocab[\"<eos>\"]])], dim=0))\n",
    "        trg_batch.append(torch.cat([torch.tensor([trg_vocab[\"<bos>\"]]), trg_item, torch.tensor([trg_vocab[\"<eos>\"]])], dim=0))\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"])\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=trg_vocab[\"<pad>\"])\n",
    "    return src_batch, trg_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Assuming you have defined src_file and trg_file paths\n",
    "dataset = TranslationDataset('europarl-v7.it-en.it', 'europarl-v7.it-en.en', src_tokenizer, trg_tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
