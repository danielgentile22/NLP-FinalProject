{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ZBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Italian sentences: 1909116\n",
      "Total English sentences: 1909116\n",
      "Italian sentence 1: Ripresa della sessione\n",
      "English sentence 1: Resumption of the session\n",
      "\n",
      "Italian sentence 2: Dichiaro ripresa la sessione del Parlamento europeo, interrotta venerdì 17 dicembre e rinnovo a tutti i miei migliori auguri nella speranza che abbiate trascorso delle buone vacanze.\n",
      "English sentence 2: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "\n",
      "Italian sentence 3: Come avrete avuto modo di constatare il grande \"baco del millennio\" non si è materializzato. Invece, i cittadini di alcuni nostri paesi sono stati colpiti da catastrofi naturali di proporzioni davvero terribili.\n",
      "English sentence 3: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "\n",
      "Italian sentence 4: Avete chiesto che si tenesse una discussione su tale tema nei prossimi giorni, nel corso della presente tornata.\n",
      "English sentence 4: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "\n",
      "Italian sentence 5: Nel frattempo è mio desiderio, come del resto mi è stato chiesto da alcuni colleghi, osservare un minuto di silenzio in memoria di tutte le vittime delle tempeste che si sono abbattute sui diversi paesi dell' Unione europea.\n",
      "English sentence 5: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "italian_file_path = 'europarl-v7.it-en.it'\n",
    "english_file_path = 'europarl-v7.it-en.en'\n",
    "\n",
    "def load_sentences(file_path):\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        sentences = file.read().split('\\n')\n",
    "    return sentences\n",
    "\n",
    "italian_sentences = load_sentences(italian_file_path)\n",
    "english_sentences = load_sentences(english_file_path)\n",
    "\n",
    "# Test data loading\n",
    "print(f\"Total Italian sentences: {len(italian_sentences)}\")\n",
    "print(f\"Total English sentences: {len(english_sentences)}\")\n",
    "\n",
    "# Print the first 5 sentences in both languages to check\n",
    "for i in range(5):\n",
    "    print(f\"Italian sentence {i+1}: {italian_sentences[i]}\")\n",
    "    print(f\"English sentence {i+1}: {english_sentences[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"Tokenizes every sentence.\"\"\"\n",
    "    return [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "\n",
    "def build_vocab(tokenized_sentences, min_frequency=256):\n",
    "    \"\"\"Builds a vocabulary with tokens appearing at least min_frequency times, including special tokens.\"\"\"\n",
    "    # Count the frequency of each token across all sentences\n",
    "    token_freqs = Counter(token for sentence in tokenized_sentences for token in sentence)\n",
    "    \n",
    "    # Filter tokens by frequency threshold\n",
    "    tokens = [token for token, freq in token_freqs.items() if freq >= min_frequency]\n",
    "    \n",
    "    # Create a vocabulary with special tokens\n",
    "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "    \n",
    "    # Assign indices to filtered tokens\n",
    "    for index, token in enumerate(tokens, start=len(vocab)):\n",
    "        vocab[token] = index\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def tokens_to_indices(tokenized_sentences, vocab):\n",
    "    \"\"\"Converts tokens to indices based on a vocabulary, using <unk> for unknown tokens.\"\"\"\n",
    "    return [[vocab.get(token, vocab[\"<unk>\"]) for token in sentence] for sentence in tokenized_sentences]\n",
    "\n",
    "def pad_sequences(sequences, vocab, max_len=32, padding_value=0, ):\n",
    "    \"\"\"Pads sequences to a specified maximum length.\"\"\"\n",
    "    # Truncate longer sequences and pad shorter ones\n",
    "    return np.array([[vocab[\"<sos>\"]] + seq[:max_len - 2] + [vocab[\"<eos>\"]] + [padding_value] * max(0, max_len - (len(seq) + 2)) for seq in sequences])\n",
    "\n",
    "def save_processed_data(filename, data):\n",
    "    \"\"\"Saves processed data to a file using pickle.\"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def load_processed_data(filename):\n",
    "    \"\"\"Loads processed data from a pickle file.\"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load data...\n",
      "Failed to load data.\n",
      "Tokenizing sentences...\n",
      "Sentences tokenized.\n",
      "Building vocab...\n",
      "Vocab built.\n",
      "Tokens to indices...\n",
      "Done.\n",
      "Padding sequences...\n",
      "Sentences padded.\n",
      "Saved data for future use.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Trying to load data...\")\n",
    "\n",
    "    # Attempt to load pre-processed data if available\n",
    "    italian_tokenized = load_processed_data('italian_tokenized.pkl')\n",
    "    english_tokenized = load_processed_data('english_tokenized.pkl')\n",
    "    italian_vocab = load_processed_data('italian_vocab.pkl')\n",
    "    english_vocab = load_processed_data('english_vocab.pkl')\n",
    "    italian_padded = load_processed_data('italian_padded.pkl')\n",
    "    english_padded = load_processed_data('english_padded.pkl')\n",
    "\n",
    "    print(\"Succesfully loaded data!\")\n",
    "\n",
    "except (FileNotFoundError, IOError):\n",
    "\n",
    "    print(\"Failed to load data.\")\n",
    "\n",
    "    # Pre-process data\n",
    "    print(\"Tokenizing sentences...\")\n",
    "    italian_tokenized = tokenize_sentences(italian_sentences)\n",
    "    english_tokenized = tokenize_sentences(english_sentences)\n",
    "    print(\"Sentences tokenized.\")\n",
    "    \n",
    "    save_processed_data('italian_tokenized.pkl', italian_tokenized)\n",
    "    save_processed_data('english_tokenized.pkl', english_tokenized)\n",
    "\n",
    "    print(\"Building vocab...\")\n",
    "    italian_vocab = build_vocab(italian_tokenized)\n",
    "    english_vocab = build_vocab(english_tokenized)\n",
    "    print(\"Vocab built.\")\n",
    "    \n",
    "    save_processed_data('italian_vocab.pkl', italian_vocab)\n",
    "    save_processed_data('english_vocab.pkl', english_vocab)\n",
    "\n",
    "    print(\"Tokens to indices...\")\n",
    "    italian_indices = tokens_to_indices(italian_tokenized, italian_vocab)\n",
    "    english_indices = tokens_to_indices(english_tokenized, english_vocab)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    print(\"Padding sequences...\")\n",
    "    italian_padded = pad_sequences(italian_indices, italian_vocab)\n",
    "    english_padded = pad_sequences(english_indices, english_vocab)\n",
    "    print(\"Sentences padded.\")\n",
    "    \n",
    "    save_processed_data('italian_padded.pkl', italian_padded)\n",
    "    save_processed_data('english_padded.pkl', english_padded)\n",
    "\n",
    "    print(\"Saved data for future use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TranslationDataset class for use with PyTorch\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            'target': torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Splitting the dataset\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(italian_padded, english_padded, test_size=0.1)\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(train_inputs, train_targets, test_size=0.1)\n",
    "\n",
    "# Create Dataset and DataLoader instances for PyTorch\n",
    "train_dataset = TranslationDataset(train_inputs, train_targets)\n",
    "val_dataset = TranslationDataset(val_inputs, val_targets)\n",
    "test_dataset = TranslationDataset(test_inputs, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10004\n"
     ]
    }
   ],
   "source": [
    "maximum = 0\n",
    "for item in italian_vocab.items():\n",
    "    if maximum < int(item[1]):\n",
    "        maximum = int(item[1])\n",
    "\n",
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10005"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(italian_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 45,  8, 45,  7,  7, 12, 66,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1, 12, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,\n",
       "        78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,\n",
       "        78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,\n",
       "        78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,  2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = [[45, 8, 45, 7, 7, 12, 66], [12]  + [78] * 200]\n",
    "\n",
    "pad_sequences(lists, english_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# Implementation:\n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq/seq2seq.py\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english_vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZBOOK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\ZBOOK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 started...\n",
      "batch 100 started...\n",
      "batch 200 started...\n",
      "batch 300 started...\n",
      "batch 400 started...\n",
      "batch 500 started...\n",
      "batch 600 started...\n",
      "batch 700 started...\n",
      "batch 800 started...\n",
      "batch 900 started...\n",
      "batch 1000 started...\n",
      "batch 1100 started...\n",
      "batch 1200 started...\n",
      "batch 1300 started...\n",
      "batch 1400 started...\n",
      "batch 1500 started...\n",
      "batch 1600 started...\n",
      "batch 1700 started...\n",
      "batch 1800 started...\n",
      "batch 1900 started...\n",
      "batch 2000 started...\n",
      "batch 2100 started...\n",
      "batch 2200 started...\n",
      "batch 2300 started...\n",
      "batch 2400 started...\n",
      "batch 2500 started...\n",
      "batch 2600 started...\n",
      "batch 2700 started...\n",
      "batch 2800 started...\n",
      "batch 2900 started...\n",
      "batch 3000 started...\n",
      "batch 3100 started...\n",
      "batch 3200 started...\n",
      "batch 3300 started...\n",
      "batch 3400 started...\n",
      "batch 3500 started...\n",
      "batch 3600 started...\n",
      "batch 3700 started...\n",
      "batch 3800 started...\n",
      "batch 3900 started...\n",
      "batch 4000 started...\n",
      "batch 4100 started...\n",
      "batch 4200 started...\n",
      "batch 4300 started...\n",
      "batch 4400 started...\n",
      "batch 4500 started...\n",
      "batch 4600 started...\n",
      "batch 4700 started...\n",
      "batch 4800 started...\n",
      "batch 4900 started...\n",
      "batch 5000 started...\n",
      "batch 5100 started...\n",
      "batch 5200 started...\n",
      "batch 5300 started...\n",
      "batch 5400 started...\n",
      "batch 5500 started...\n",
      "batch 5600 started...\n",
      "batch 5700 started...\n",
      "batch 5800 started...\n",
      "batch 5900 started...\n",
      "batch 6000 started...\n",
      "batch 6100 started...\n",
      "batch 6200 started...\n",
      "batch 6300 started...\n",
      "batch 6400 started...\n",
      "batch 6500 started...\n",
      "batch 6600 started...\n",
      "batch 6700 started...\n",
      "batch 6800 started...\n",
      "batch 6900 started...\n",
      "batch 7000 started...\n",
      "batch 7100 started...\n",
      "batch 7200 started...\n",
      "batch 7300 started...\n",
      "batch 7400 started...\n",
      "batch 7500 started...\n",
      "batch 7600 started...\n",
      "batch 7700 started...\n",
      "batch 7800 started...\n",
      "batch 7900 started...\n",
      "batch 8000 started...\n",
      "batch 8100 started...\n",
      "batch 8200 started...\n",
      "batch 8300 started...\n",
      "batch 8400 started...\n",
      "batch 8500 started...\n",
      "batch 8600 started...\n",
      "batch 8700 started...\n",
      "batch 8800 started...\n",
      "batch 8900 started...\n",
      "batch 9000 started...\n",
      "batch 9100 started...\n",
      "batch 9200 started...\n",
      "batch 9300 started...\n",
      "batch 9400 started...\n",
      "batch 9500 started...\n",
      "batch 9600 started...\n",
      "batch 9700 started...\n",
      "batch 9800 started...\n",
      "batch 9900 started...\n",
      "batch 10000 started...\n",
      "batch 10100 started...\n",
      "batch 10200 started...\n",
      "batch 10300 started...\n",
      "batch 10400 started...\n",
      "batch 10500 started...\n",
      "batch 10600 started...\n",
      "batch 10700 started...\n",
      "batch 10800 started...\n",
      "batch 10900 started...\n",
      "batch 11000 started...\n",
      "batch 11100 started...\n",
      "batch 11200 started...\n",
      "batch 11300 started...\n",
      "batch 11400 started...\n",
      "batch 11500 started...\n",
      "batch 11600 started...\n",
      "batch 11700 started...\n",
      "batch 11800 started...\n",
      "batch 11900 started...\n",
      "batch 12000 started...\n",
      "batch 12100 started...\n",
      "batch 12200 started...\n",
      "batch 12300 started...\n",
      "batch 12400 started...\n",
      "batch 12500 started...\n",
      "batch 12600 started...\n",
      "batch 12700 started...\n",
      "batch 12800 started...\n",
      "batch 12900 started...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m---> 86\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     40\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(len(italian_vocab), embedding_size=64, hidden_size=128, num_layers=1, p=0.5).to(device)\n",
    "decoder = Decoder(len(english_vocab), embedding_size=64, hidden_size=128, output_size=len(english_vocab), num_layers=1, p=0.5).to(device)\n",
    "\n",
    "# Initialize Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Loss function\n",
    "pad_idx = 0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"batch {i} started...\")\n",
    "        src = batch['input'].to(device)\n",
    "        trg = batch['target'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Output shape: (trg_len, batch_size, output_dim)\n",
    "        # Target shape: (trg_len, batch_size)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch['input'].to(device)\n",
    "            trg = batch['target'].to(device)\n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43menglish_vocab\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'english_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "len(english_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
